1
00:00:00,000 --> 00:00:03,567
In the next three segments we will change
gears a little bit and talk about the

2
00:00:03,567 --> 00:00:07,224
definition of a PRG. This definition is a
really good way to think of a PRG. And we

3
00:00:07,224 --> 00:00:10,784
will see many applications for this
definition. So consider a PRG with

4
00:00:10,784 --> 00:00:15,943
keyspace A that ouputs N bit strings. Our
goal is to define, what does it mean for

5
00:00:15,943 --> 00:00:20,592
the output of the generator to be
indistinguishable from random? In other

6
00:00:20,592 --> 00:00:25,687
words, we're gonna define a distribution
that basically is defined by choosing a

7
00:00:25,687 --> 00:00:30,781
random key in the keyspace. Remember that
a arrow with R above it means choosing

8
00:00:30,781 --> 00:00:35,813
uniformly from the set script K. And then
we output, basically, the output of the

9
00:00:35,813 --> 00:00:40,710
generator. And what we'd like to say. Is
that this distribution. This distribution

10
00:00:40,710 --> 00:00:44,757
of pseudo random strings is
indistinguishable from a truly uniform

11
00:00:44,757 --> 00:00:49,927
distribution. In other words, if we just
choose a truly uniform string in 01 to the

12
00:00:49,927 --> 00:00:55,096
N and simply output this string, we'd like
to say that these two distributions are

13
00:00:55,096 --> 00:01:00,139
indistinguishable from one another. Now if
you think about it, this sounds really

14
00:01:00,139 --> 00:01:05,246
surprising because if we draw a circle
here of all possible strings in 01 to the

15
00:01:05,246 --> 00:01:10,541
N, then the uniform distribution basically
can ouput any of these strings with equal

16
00:01:10,541 --> 00:01:15,161
probability. That's the definition of the
uniform distribution. However a

17
00:01:15,161 --> 00:01:20,661
pseudo-random distribution generated by
this generator G. Because the seed space

18
00:01:20,661 --> 00:01:25,673
is so small, the set of possible outputs
is really, really small, it's tiny inside

19
00:01:25,673 --> 00:01:30,623
of, 01 to the N. And this is really all
that the generator can output. And yet,

20
00:01:30,623 --> 00:01:35,698
what we're arguing is that an adversary
who looks at the output of the generator

21
00:01:35,698 --> 00:01:40,836
in this tiny set can't distinguish it from
the output of the uniform distribution

22
00:01:40,836 --> 00:01:45,703
over the entire set. I think that's the
property that we're actually shooting for.

23
00:01:45,703 --> 00:01:50,026
So to understand how to define this
concept of indistinguishability from

24
00:01:50,026 --> 00:01:54,350
random, we need the concept of a
statistical test. So, let me define what a

25
00:01:54,350 --> 00:01:59,030
statistical test on 01 to the N is. I'm
gonna define these statistical tests by

26
00:01:59,030 --> 00:02:03,924
the letter A. And the statistical test is
basically an algorithm that takes its

27
00:02:03,924 --> 00:02:08,486
inputs and N bit string, and simply
outputs zero or one. Now I'll tell you

28
00:02:08,486 --> 00:02:13,110
that zero, we're gonna think of it as
though the statistical test said, the

29
00:02:13,110 --> 00:02:18,150
input you gave me is not random. And one,
we're going to think of it as saying that

30
00:02:18,150 --> 00:02:23,196
the imput you gave me actually is random.
Okay, so all this statistical test does is

31
00:02:23,196 --> 00:02:27,939
it basically takes the input x that was
given to it, the n bit string that was

32
00:02:27,939 --> 00:02:32,803
given to it, and decides whether it looks
random or it doesn't look random. Let's

33
00:02:32,803 --> 00:02:37,849
look at a couple of examples. So the first
example basically will use the fact that

34
00:02:37,849 --> 00:02:42,835
for a random string, the number of zeros
is roughly equal to the number of ones in

35
00:02:42,835 --> 00:02:47,248
that string. In other words, the
statistical test is going to say one. If

36
00:02:47,248 --> 00:02:54,514
and only if basically the number of zeros
in the given string X minus the number of

37
00:02:54,514 --> 00:02:59,837
1's in the given string X. These numbers
are not too far apart. In other words, the

38
00:02:59,837 --> 00:03:05,225
difference between the number of 0's and
the number of 1's. Let's just say is less

39
00:03:05,225 --> 00:03:10,614
than ten times square root of n. Okay If
the difference is less than ten times, the

40
00:03:10,614 --> 00:03:15,082
statistical test will say hey the string X looks random. If the

41
00:03:15,082 --> 00:03:20,602
difference happens to be much bigger than
ten times square root of n, that starts to

42
00:03:20,602 --> 00:03:25,991
look suspicious and the test,
hey the string you gave me does not

43
00:03:25,991 --> 00:03:31,403
look random. A statistical test. Let's
look at another similar example. We'll say

44
00:03:31,403 --> 00:03:36,785
here, the statistical test will say one.
If and only if say the number of times

45
00:03:36,785 --> 00:03:41,838
that we have two consecutive zeros. Inside
of X. But let's think about this for a

46
00:03:41,838 --> 00:03:46,766
second. This basically again counts. In
this string of, n bits. It counts a number

47
00:03:46,766 --> 00:03:51,734
of times that we see the pattern zero,
zero. Two consecutive zeros. Well for a

48
00:03:51,734 --> 00:03:56,804
random string. We will expect to see 0,0
as probability one fourth. And there for

49
00:03:56,804 --> 00:04:03,535
in a random string. We'll expect about N
over four 0,0's. Yeah, N over four blocks

50
00:04:03,535 --> 00:04:08,133
a 0,0. And so, what the statistical test
will do is it will say, well, if the

51
00:04:08,133 --> 00:04:12,604
number of zero zeros is roughly N over
four. In other words, the difference

52
00:04:12,604 --> 00:04:17,254
between the number and N over four,
is, say, less than ten square root of n,

53
00:04:17,254 --> 00:04:22,005
then we will say that X looks random. And
if the gap is much bigger than N over

54
00:04:22,005 --> 00:04:26,309
four, we'll say, hey, this string doesn't
really look random. And then the

55
00:04:26,309 --> 00:04:31,097
statistical test will output zero, okay?
So here are two examples of statistical

56
00:04:31,097 --> 00:04:35,703
tests that basically, for random strings,
they will output one with very high

57
00:04:35,703 --> 00:04:40,067
probability. But for strings that, you
know, don't look random, for example,

58
00:04:40,067 --> 00:04:44,856
think of the all zero string. So the all
zero string, neither one of these tests

59
00:04:44,856 --> 00:04:49,986
will output, one. And in fact, the all
zero string does not look random. Let's

60
00:04:49,986 --> 00:04:55,098
look at one more example of the
statistical test just to kinda show you

61
00:04:55,098 --> 00:05:00,993
the, basically statistical test can pretty
much do whatever they want. So here's the

62
00:05:00,993 --> 00:05:06,745
third example. Let's say that statistical
test output one if an only if I say the

63
00:05:06,745 --> 00:05:12,497
biggest blocks what we'll call this the
Maximum Run of zero inside of the string

64
00:05:12,497 --> 00:05:17,950
x, this is basically the longest sequence
of zero inside of the string x. In a

65
00:05:17,950 --> 00:05:23,336
random string you expect the longest
sequence of zeros to be roughly of length

66
00:05:23,336 --> 00:05:28,449
log N. So we'll say if the longest
sequence of zero happens to be less than

67
00:05:28,449 --> 00:05:32,814
ten times log N Then this test will say
that X was random. But if, all of a

68
00:05:32,814 --> 00:05:37,160
sudden, we see a run of zeros that, say,
is much bigger than ten log N, then the

69
00:05:37,160 --> 00:05:41,733
statistical test will say, the string is
not random, okay? So this is another crazy

70
00:05:41,733 --> 00:05:46,192
thing that the statistical test will do.
By the way, you notice that if you give

71
00:05:46,192 --> 00:05:50,444
this test, the all one string. So one,
one, one, one, one. This test will also

72
00:05:50,444 --> 00:05:55,370
output one. In other words this test will
think that the all one string is random.

73
00:05:55,370 --> 00:05:59,735
Even though it's not. Yeah, even though
one string is not particularly random.

74
00:05:59,735 --> 00:06:04,068
Okay, so statistical tests don't have to
get things right. They can do whatever

75
00:06:04,068 --> 00:06:08,149
they like. They can test, they can decide
to output random or not. You know, zero or

76
00:06:08,149 --> 00:06:11,776
one, however they like. And similarly,
there are many, many, many, many other

77
00:06:11,776 --> 00:06:15,857
statistical tests. There are literally
hundreds of statistical tests that one can

78
00:06:15,857 --> 00:06:19,737
think of. And I can tell you that in the
old days, basically, the way you would

79
00:06:19,737 --> 00:06:23,663
define. That something looks random. As
you would say, hey, here's a battery of

80
00:06:23,663 --> 00:06:27,754
statistical tests. And all of them said
that this string looks random. Therefore,

81
00:06:27,754 --> 00:06:31,896
we say that this generator that generated
the string is good generator. In other

82
00:06:31,896 --> 00:06:36,091
words, this definition, then, uses a fixed
set of statistical tests, is actually not

83
00:06:36,091 --> 00:06:40,364
a good definition for security, but more
generally, for crytpo. But before we talk

84
00:06:40,364 --> 00:06:45,290
about actually defining security, the next
thing we talk about is how do we evaluate

85
00:06:45,290 --> 00:06:50,040
whether a statistical test is good or not?
So to do that, we define the concept of

86
00:06:50,040 --> 00:06:54,321
advantage. And so let me define the
advantage. So here we have a generator

87
00:06:54,321 --> 00:06:59,188
that outputs N bit strings. And we have a
statistical tests on N bit strings. And we

88
00:06:59,188 --> 00:07:04,578
define the advantage of this generator,
as denoted by advantage sub PRG,

89
00:07:04,578 --> 00:07:10,313
the advantage of the statistical test A
relative to the generator g. I'll define

90
00:07:10,313 --> 00:07:16,121
it as follows, basically the difference
between two quantities. The first quantity

91
00:07:16,121 --> 00:07:21,679
is basically, we ask how likely is this
statistical test to output one. When we

92
00:07:21,679 --> 00:07:27,007
give it a pseudo random string just like
here K is chosen uniformly from the C

93
00:07:27,007 --> 00:07:32,403
space we ask how likely is the statistical
test to output one when we give it a

94
00:07:32,403 --> 00:07:37,799
pseudo random output generated by the
generator verses now we ask how likely is

95
00:07:37,799 --> 00:07:43,532
the statistical test to output one when we
give it a truly random string. So here are

96
00:07:43,532 --> 00:07:48,174
is truly random in zero random one to the
n. Okay, and yeah. We look at the

97
00:07:48,174 --> 00:07:52,447
difference between these two quantities.
Now you realize because these are

98
00:07:52,447 --> 00:07:56,720
differences of probabilities this
advantage is always going to lie in the

99
00:07:56,720 --> 00:08:00,733
interval zero, one. So let's think a
little bit about what this advantage

100
00:08:00,733 --> 00:08:05,138
actually means. So first of all if the
advantage happens to be close to one. Well

101
00:08:05,138 --> 00:08:09,580
what does that mean. That means that
somehow, the statistical test A behaves

102
00:08:09,580 --> 00:08:14,620
differently when we gave it pseudo-random
inputs, when we gave it the output of the

103
00:08:14,620 --> 00:08:19,175
generator, for when we gave it truly
random inputs, right? It somehow behaved

104
00:08:19,175 --> 00:08:23,790
differently. In one case, it output one
with a certain probability. And in the

105
00:08:23,790 --> 00:08:28,344
other case, it output one with a very
different probability, okay? So somehow,

106
00:08:28,344 --> 00:08:32,778
it was able to behave differently. And
what they really means is that the

107
00:08:32,778 --> 00:08:37,211
statistical test could basically
distinguish the output of the generator

108
00:08:37,211 --> 00:08:42,877
from random. Okay, so in some sense we'll
say that this statistical test broke the

109
00:08:42,877 --> 00:08:47,053
generator G because it was able to
distinguish the output from random.

110
00:08:47,053 --> 00:08:51,975
However if the advantage is close to zero
Well what does that mean. That means that

111
00:08:51,975 --> 00:08:56,451
basically the statistical tests behaves
pretty much the same on pseudo random

112
00:08:56,451 --> 00:09:01,222
inputs. As it does on truly random inputs.
And basically there we would say that A

113
00:09:01,222 --> 00:09:05,773
could not distinguish the generator from
random. Okay, so this sum gives you a

114
00:09:05,773 --> 00:09:10,289
little bit of intuition about why this
concept of advantage is important. It

115
00:09:10,289 --> 00:09:15,222
basically tells us whether A was able to
break the generator, namely distinguish it

116
00:09:15,222 --> 00:09:19,917
from random, or not able to break it. So
let's look, first of all, at a very silly

117
00:09:19,917 --> 00:09:24,671
example. Suppose we have a statistical
test A that simply ignores its inputs and

118
00:09:24,671 --> 00:09:29,496
always outputs zero. Okay. Always output
zero. What do you think of the advantage

119
00:09:29,496 --> 00:09:33,742
of this statistical test relative to a
generator G? So, I hope everybody

120
00:09:33,742 --> 00:09:37,932
say the advantage is zero, let
me just explain, why that's the case,

121
00:09:37,932 --> 00:09:41,791
well, if the statistical test, always outputs, zero, that

122
00:09:41,791 --> 00:09:45,982
means, pseudo random inputs, it will never
output one, so, the probability that

123
00:09:45,982 --> 00:09:50,282
outputs one, is zero. Similarly, when we
give a truly random input, it still will

124
00:09:50,282 --> 00:09:54,527
never output one, and, so, the probability
that outputs one, is zero,

125
00:09:54,527 --> 00:09:58,772
and, so, zero minus zero is zero, so, its
advantage is zero, so, basically, and, a

126
00:09:58,772 --> 00:10:03,128
statistical test that ignores its, its
input, does not able to distinguish, truly

127
00:10:03,128 --> 00:10:07,441
random inputs, from, a pseudo random
input, obviously. Okay, now let's look at

128
00:10:07,441 --> 00:10:12,861
a more interesting example. So suppose we
have a generator G that satisfies a funny

129
00:10:12,861 --> 00:10:17,671
property. It so happens that for
two-thirds of the keys The first bit

130
00:10:17,671 --> 00:10:22,892
of the output of the generator happens to
be one, okay? So if I choose a random key

131
00:10:22,892 --> 00:10:28,176
with probability two-thirds, the generator
will output one as its first bit, okay? So

132
00:10:28,176 --> 00:10:33,333
that's the property of the generator that
we're looking at. Now, let's look at the

133
00:10:33,333 --> 00:10:37,790
following statistical test. The
statistical test basically says, if the

134
00:10:37,790 --> 00:10:42,883
most signifigant bit of the string you
gave me is one, I'm gonna say one, meaning

135
00:10:42,883 --> 00:10:48,167
I think it's random. If the most signigant
bit of the stream you gave me is not one,

136
00:10:48,167 --> 00:10:53,969
zero, I'm gonna say zero. Okay
so now my question to you is what is the

137
00:10:53,969 --> 00:10:59,625
advantage of this statistical test on the
generator G? Okay, so remember I just

138
00:10:59,625 --> 00:11:04,552
wrote down the definition here again. And
I'll let you think about this for a

139
00:11:04,552 --> 00:11:09,933
second. So let me explain. Suppose we give
the statistical tests pseudo random

140
00:11:09,933 --> 00:11:14,568
inputs. By definition of G, we know that
with probability two-thirds, the first

141
00:11:14,568 --> 00:11:19,625
bits in the inputs will start with the bit
one. But if it starts with a bit one, then

142
00:11:19,625 --> 00:11:24,320
the statistical test will output one. In
other words, the probability that this

143
00:11:24,320 --> 00:11:29,196
statistical test outputs one is exactly
two-thirds. Now let's look at the case of

144
00:11:29,196 --> 00:11:33,831
a random string. If I give you a random
string, how likely is it that the most

145
00:11:33,831 --> 00:11:38,346
signifigant bits of the random string is
one? Well, for a random string, that

146
00:11:38,346 --> 00:11:43,342
happens exactly half the time, and so in
this case the statistical test will output

147
00:11:43,342 --> 00:11:47,918
one, with probability one-half. And so the
overall advantage is one-sixth, and

148
00:11:47,918 --> 00:11:52,553
one-sixth is actually a non-negligible
number, that's actually a fairly large

149
00:11:52,553 --> 00:11:57,490
number, which basically means that this a
was able to distinguish the output. We'll

150
00:11:57,490 --> 00:12:03,314
say that a breaks the generator G with
advantage 1/6. Okay, which basically

151
00:12:03,314 --> 00:12:08,292
means that this generator is no good, is
broken. Okay, so now that we understand

152
00:12:08,292 --> 00:12:13,521
what statistical tests are, we can go
ahead and define, what is a secure

153
00:12:13,521 --> 00:12:19,191
pseudo-random generator. So, basically, we
say that, as generator G is secure, if

154
00:12:19,191 --> 00:12:24,935
essentially no efficient, statistical
tests can distinguish its output from

155
00:12:24,935 --> 00:12:30,311
random. More precisely, what we'll say is
that, basically for all efficient

156
00:12:30,311 --> 00:12:36,423
statistical tests, a... Statistical tests,
a... It so happens that if I look at the

157
00:12:36,423 --> 00:12:42,496
advantage. Of the statistical test E
relative to G. This advantage basically is

158
00:12:42,496 --> 00:12:47,246
negligible. So, in other words,
it's very close to zero, and as a result,

159
00:12:47,246 --> 00:12:52,103
this, statistical test was not able to
distinguish the output from random, and

160
00:12:52,103 --> 00:12:56,961
that has to be true for all statistical
tests. So, this is a very, very pretty and

161
00:12:56,961 --> 00:13:01,211
elegant definition, that says that a
generator is secure, not only if a

162
00:13:01,211 --> 00:13:06,129
particular battery of statistical tests
says that the output looks random, but, in

163
00:13:06,129 --> 00:13:10,929
fact, all efficient statistical tests will
say the output looks random. Okay? One

164
00:13:10,929 --> 00:13:15,992
thing I'd like to point out is, that the
restriction to efficient statistical tests

165
00:13:15,992 --> 00:13:20,934
is actually necessary. If we ask that all
statistical tests, regardless of whether

166
00:13:20,934 --> 00:13:25,836
they're efficient or not, not be able to
distinguish the output from random. Then

167
00:13:25,836 --> 00:13:30,469
in fact, that can not be satisfied. So in
other words if we took out the

168
00:13:30,469 --> 00:13:34,826
requirements that the test be efficient.
Then this definition would be

169
00:13:34,826 --> 00:13:39,484
unsatisfiable. And I'll leave this as a
simple puzzle for you to think about. But

170
00:13:39,484 --> 00:13:43,851
basically the fact is that restricting
this definition into only efficient

171
00:13:43,851 --> 00:13:48,625
statistical tests is actually necessary
for this to be satisfiable. So now that we

172
00:13:48,625 --> 00:13:53,341
have a definition, the next question is
can we actually construct a generator and

173
00:13:53,341 --> 00:13:57,533
then prove that it is in fact a secure
PRG. In other words, prove that no

174
00:13:57,533 --> 00:14:02,366
efficient statistical test can distinguish
its output from random. And it turns out

175
00:14:02,366 --> 00:14:07,022
that the answer is we actually can't. In
fact, it's not known. If there are any

176
00:14:07,022 --> 00:14:12,627
probably secure PRG's. Then I will just
say very briefly the reason is that if you

177
00:14:12,627 --> 00:14:18,301
could prove that a particular generator is
secure that would actually imply that P

178
00:14:18,301 --> 00:14:23,455
is not equal to NP. And I don't want to
dwell on this. Because I don't want to

179
00:14:23,455 --> 00:14:28,706
assume that you guys know what P and NP
are. But I'll tell you as a simple fact

180
00:14:28,706 --> 00:14:33,420
that in fact in P is equal to NP. Then
it's very easy to show that there are no

181
00:14:33,420 --> 00:14:37,858
secure PRGs. And so if you could prove to
me that a particular PRG is secure, that

182
00:14:37,858 --> 00:14:41,913
would imply that P is not equal to
NP. Again, I will leave this to

183
00:14:41,913 --> 00:14:45,858
you as a simple puzzle to think about.
But, even though we can't actually

184
00:14:45,858 --> 00:14:50,297
rigorously prove that a particular PRG is
secure, we still have lots and lots and

185
00:14:50,297 --> 00:14:54,406
lots of heuristic candidates, and we even
saw some of those in the previous

186
00:14:54,406 --> 00:14:59,289
segments. Okay now that we understand what
is a secure PRG. I want to talk a little

187
00:14:59,289 --> 00:15:03,558
bit about some applications and
implications of this definition. And so

188
00:15:03,558 --> 00:15:08,308
the first thing I want to show you is that
in fact a secure PRG is necessarily

189
00:15:08,308 --> 00:15:12,884
unpredictable. In a previous segment, we
talked about what it means for a generator

190
00:15:12,884 --> 00:15:17,114
to be unpredictable. And we said that,
basically, what that means is that, given

191
00:15:17,114 --> 00:15:21,508
a prefix of the output generator, it's
impossible to predict the next bit of the

192
00:15:21,508 --> 00:15:25,902
output. Okay, so we'd like to show that if
a generator is secure, then necessarily,

193
00:15:25,902 --> 00:15:30,176
it means it's unpredictable. And so the
only way we're gonna do that is using the

194
00:15:30,176 --> 00:15:34,254
contrapositive. That is, we're gonna say
that if you give me a generator that is

195
00:15:34,254 --> 00:15:37,971
predictable, then necessarily, it's
insecure. In other words, necessarily, I

196
00:15:37,971 --> 00:15:42,050
can distinguish it from random. And so
let's see, this is actually a very simple

197
00:15:42,050 --> 00:15:46,077
fact. And so let's see how we would do
that. So suppose you give me a predictor.

198
00:15:46,077 --> 00:15:50,000
In other words, suppose you give me an
efficient algorithm, such that, in fact,

199
00:15:50,000 --> 00:15:54,234
if I give this algorithm the output of the
generator, but I give it only the first

200
00:15:54,234 --> 00:15:58,599
I-bits of the outputs. It's able to
predict the next bit of the output. In

201
00:15:58,599 --> 00:16:03,664
other words given the first I-bit it's
able to predict the I plus first bit. And

202
00:16:03,664 --> 00:16:08,827
it does that with a certain probability.
So let's say if we choose a random k. From

203
00:16:08,827 --> 00:16:13,367
the keyspace. Then, clearly, a done predictor would be able to predict the

204
00:16:13,367 --> 00:16:18,138
next bit with probability one-half, simply
just guess the bits. You'll be right with

205
00:16:18,138 --> 00:16:22,391
probability one-half. However, this
algorithm A is able to predict the next

206
00:16:22,391 --> 00:16:27,053
bit with probability half with epsilon. So
it's bound to the way. From a half. And,

207
00:16:27,053 --> 00:16:31,726
in fact, we require that this by true for
some non negligible epsilon. So, for

208
00:16:31,726 --> 00:16:36,338
example, epsilon =1/1000 would already be a
dangerous predictor, because it can

209
00:16:36,338 --> 00:16:40,949
predict the next bits, given a prefix,
with non negligible advantage. Okay, so

210
00:16:40,949 --> 00:16:45,533
suppose we have such an algorithm. Let's
see that we can use this algorithm to

211
00:16:45,533 --> 00:16:50,010
break our generator. In other words to
show that a generator is distinguishable

212
00:16:50,010 --> 00:16:54,146
from random and therefore, is insecure. So
what we'll do is we'll define a

213
00:16:54,146 --> 00:16:59,463
statistical test. So, let's define the
statistical test B as follows. Basically,

214
00:16:59,463 --> 00:17:04,979
B, given a string, x, what it will do, is
it will simply run algorithm A on the

215
00:17:04,979 --> 00:17:10,711
first I-bit of the string x that it was
given. And, statistical test b is simply

216
00:17:10,711 --> 00:17:16,662
gonna ask, was a successful in predicting
the I-plus first bit of the string? If it

217
00:17:16,662 --> 00:17:22,579
was successful, then it's gonna output
one. And if it wasn't successful, then

218
00:17:22,579 --> 00:17:28,154
it's gonna output zero. Okay. This our
statistical task. Let's put it in a box So

219
00:17:28,154 --> 00:17:33,407
we can take it wherever we like. And we
can run the statistical test on any N bit

220
00:17:33,407 --> 00:17:38,466
string that's given to us as inputs. So
now, let's look at what happens. Suppose

221
00:17:38,466 --> 00:17:43,524
we give the statistical test, a truly
random string. So a truly random string R.

222
00:17:43,524 --> 00:17:48,583
And we ask, what is the probability that
the statistical test outputs one? Well,

223
00:17:51,112 --> 00:17:53,642
for a truly random string, the I+1
bit is totally independent of the first

224
00:17:53,642 --> 00:17:58,765
I-bits. So whatever this algorithm is
gonna output is completely independent of

225
00:17:58,765 --> 00:18:04,231
what's, I+1 bit of the string R is.
And so whatever A outputs the probability

226
00:18:07,162 --> 00:18:10,092
is going to be equal to some random bit X I+1. Random independent bit X I+1, that

227
00:18:10,092 --> 00:18:14,754
probability is exactly 1/2. In other
words, algorithm a simply has no

228
00:18:14,754 --> 00:18:19,620
information about what the bit X I+1 is, and so necessarily, the probability is

229
00:18:19,620 --> 00:18:24,254
able to predict X I+1 is exactly one half. On the other hand, let's look at

230
00:18:24,254 --> 00:18:28,946
what happens when we give our statistical
tests a pseudo-random sequence, okay. So

231
00:18:28,946 --> 00:18:33,521
now we're going to run the statistical
test on the output of the generator, and

232
00:18:33,521 --> 00:18:37,866
we ask how likely is it to output one.
Well, by definition of A, we know that

233
00:18:37,866 --> 00:18:42,326
when we give it the first I bits of the
output of the generator, it's able to

234
00:18:42,326 --> 00:18:46,851
predict the next bit with probability 1/2 + epislon. So in this case our

235
00:18:46,851 --> 00:18:51,657
statistical test B will output one with
probability greater than 1/2 + epsilon

236
00:18:51,657 --> 00:18:57,700
And basically what this means, is if we look at the advantage of our

237
00:18:57,700 --> 00:19:04,207
statistical tests over the generator G
it's basically, the difference between

238
00:19:04,207 --> 00:19:09,648
this quantity and that quantity. There's a
difference between the two. You can see

239
00:19:09,648 --> 00:19:14,514
that it's clearly greater than an epsilon.
So what this means is that if algorithm A

240
00:19:14,514 --> 00:19:18,917
is able to predict the next bits with
advantage epsilon, then algorithm B is

241
00:19:18,917 --> 00:19:23,533
able to distinguish the output of the
generator with advantage epsilon. Okay? So

242
00:19:23,533 --> 00:19:28,696
if A is a good predictor, B is a good
statistical test that break the generator.

243
00:19:28,696 --> 00:19:33,859
And as we said, the counter-positive of
that is that if G is a secure generator,

244
00:19:33,859 --> 00:19:38,960
then there are no good statistical tests.
And as a result, there are no predictors.

245
00:19:38,960 --> 00:19:43,756
Okay? Which means that the generator is,
as we said, unpredictable. Okay, so, so

246
00:19:43,756 --> 00:19:48,361
far, what we've seen is that if the
generator is secure, necessarily, it's

247
00:19:48,361 --> 00:19:53,030
impossible to predict the I+1 bit,
given first I bits.

248
00:19:53,030 --> 00:19:57,890
Now there's a very elegant and remarkable
theorem Yao back in 1982. They

249
00:19:57,890 --> 00:20:02,623
chose it, in fact the converse is also
true. In other words, if I give you a

250
00:20:02,623 --> 00:20:07,675
generator that's unpredictable, so you
cannot predict the I+1 bits from the

251
00:20:07,675 --> 00:20:12,453
first I bits, and that's true for all I.
That generator, in fact, is secure. Okay,

252
00:20:12,453 --> 00:20:17,021
so let me state the theorem a little bit
more precicely. So here we have our

253
00:20:17,021 --> 00:20:21,949
generator that outputs n bit outputs. The
theorem says the following, basically for

254
00:20:21,949 --> 00:20:26,517
all bit positions, it's impossible to
predict I+1 bit of the output

255
00:20:26,517 --> 00:20:30,905
given the first I bit. And that's true for
all I. In other words, again, the

256
00:20:30,905 --> 00:20:35,181
generator is unpredictable for all bit
positions. Then, that, in fact, implies

257
00:20:35,181 --> 00:20:38,969
that the generator is a secure PRG.
I want paraphrase this in English,

258
00:20:39,123 --> 00:20:43,321
and so the way to kinda interpret this
result is to say that it's basically these

259
00:20:43,321 --> 00:20:47,570
next bit predictors. These predictors that
try to predict the I+1 bit given the

260
00:20:47,570 --> 00:20:51,410
first I bits. If they're not able to
distinguish G from random, then, in fact,

261
00:20:51,410 --> 00:20:55,125
no statistical test is going to
distinguish G from random. So kind of next

262
00:20:55,125 --> 00:20:58,804
bit predictors are in some sense
universal, predictors, when it comes to

263
00:20:58,804 --> 00:21:02,334
distinguishing things from random. This
theorem, by the way, it's not too

264
00:21:02,334 --> 00:21:06,113
difficult to prove, but there's a very
elegant idea behind its proof. I'm not

265
00:21:06,113 --> 00:21:10,090
gonna do the proof here, but I encourage
you to think about this as a puzzle, try

266
00:21:10,090 --> 00:21:14,017
to kind of try to prove this theorem
yourself. Let me show you kind of one cute

267
00:21:14,017 --> 00:21:19,101
implication of this theorem. So let me ask
you the following question. Suppose I give

268
00:21:19,101 --> 00:21:24,417
you a generator and I tell you that given
the last bit of the output. It's easy to

269
00:21:24,417 --> 00:21:28,963
predict the first bit of the outputs,
okay? So given the last end bits, you can

270
00:21:28,963 --> 00:21:33,627
compute the first end bits. That's kind of
the opposite of predictability, right?

271
00:21:33,627 --> 00:21:38,469
Predictability mean given the first
bit, you can produce the next bits. Here,

272
00:21:38,469 --> 00:21:43,133
given the last bits, you can produce the
first ones. And my question to you, does

273
00:21:43,133 --> 00:21:47,266
that mean that the generator is
predictable? Can you somehow, from this

274
00:21:47,266 --> 00:21:52,310
fact, still build a predictor for this
generator? This is kind of a simple

275
00:21:52,310 --> 00:21:56,898
application of Yao theorem let me explain to
you the answer is actually yes let me

276
00:21:56,898 --> 00:22:02,074
explain why how do we build this generator
well, actually we're not going to build it

277
00:22:02,074 --> 00:22:06,661
I'm going to show you that the generator
exists. Well because an over two bits

278
00:22:06,661 --> 00:22:11,661
first an over two bits doesn't necessarily
mean that the generator here let me write

279
00:22:11,661 --> 00:22:16,613
them this way what it means is that g is
not secure. Because just as we did before

280
00:22:16,613 --> 00:22:22,043
it's very easy to build a statistical test
that will distinguish the output of G from

281
00:22:22,043 --> 00:22:27,156
uniform. So G is not secure. But if G Is not
secure, by Yao's Theorem, that means that

282
00:22:27,156 --> 00:22:32,396
G is predectible. So in other words, there
exists some I for which given the first I

283
00:22:32,396 --> 00:22:37,257
bits of the output, you can build the I+1 bits of the output. Okay, so

284
00:22:37,257 --> 00:22:42,371
even though I can't quite point to you a
predicter, we know that a predicter must

285
00:22:42,371 --> 00:22:46,529
exist. So that's a one cute simple
application of Yao theorem. Now before we

286
00:22:46,693 --> 00:22:50,856
end the segment I want to kind of,
generalize a little bit of what we did.

287
00:22:50,856 --> 00:22:54,964
And introduce a little bit of important
notation that's going to be useful

288
00:22:54,964 --> 00:22:58,516
actually throughout. So, we're gonna
generalize the concept of

289
00:22:58,516 --> 00:23:02,889
indistinguishability from uniform, to
indistinguishability of two general

290
00:23:02,889 --> 00:23:07,143
distributions. So, suppose I give you p1
and p2, and we ask, can these two

291
00:23:07,143 --> 00:23:11,636
distributions be distinguished? And so
we'll say that the distributions are

292
00:23:11,636 --> 00:23:16,602
computationally indistinguishable, and
we'll denote this by p1, a squiggly p. P2.

293
00:23:16,602 --> 00:23:22,908
This means that, in polynomial time, P1
cannot be distinguished from P2. And we'll

294
00:23:22,908 --> 00:23:28,582
say that they're indistinguishable,
basically, just as before

295
00:23:28,582 --> 00:23:35,759
if basically for all, efficient, statistical
tests, statistical tests. A it so happens

296
00:23:35,759 --> 00:23:44,160
that if I sample from the distribution P1.
And I give the output to A. Versus if I

297
00:23:44,160 --> 00:23:51,208
sample from the distribution P2, and I
give the sample to A. Then basically A

298
00:23:51,208 --> 00:23:55,414
behaves the same in both cases. In
another-wards the difference between these

299
00:23:55,414 --> 00:23:59,893
two probabilities is negligible. And this
has to be true for all statistical tests.

300
00:23:59,893 --> 00:24:04,577
For all efficient statistical tests. Okay?
So if this is the case then we say that,

301
00:24:04,577 --> 00:24:09,530
well A couldn't distinguish it's advantage
in distinguishing two distributions is

302
00:24:09,530 --> 00:24:14,240
negligible and if that's true for all
efficient statistical tests then we say

303
00:24:14,240 --> 00:24:18,649
that the distributions are basically
computationaly indistinguishable,

304
00:24:18,649 --> 00:24:23,481
because an efficient algorithm cannot
distinguish them. And just to show you how

305
00:24:23,481 --> 00:24:28,372
useful this notation is, basically using
this notation the definition of security

306
00:24:28,372 --> 00:24:32,810
for PRG just says. That if I give you a
pseudo-random distribution. In other

307
00:24:32,810 --> 00:24:37,159
words, I choose K at random, and that
outputs a G of K. That distribution is

308
00:24:37,159 --> 00:24:41,978
computationally indistinguishable from the
uniform distribution. So you can see this,

309
00:24:41,978 --> 00:24:46,269
this very simple notation captures the
whole definition of pseudo-random

310
00:24:46,269 --> 00:24:50,795
generators. Okay, so we're gonna make use
of this notation. In the next segment,

311
00:24:50,795 --> 00:24:54,439
when we define, what does it mean for a
cipher to be secure.
